{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from numpy import genfromtxt\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.mixture import GMM\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn import svm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 162
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_data(filename):\n",
      "    \"\"\" Load CSV into numpy array \"\"\"\n",
      "    return np.genfromtxt(open(filename,'rb'), delimiter=',')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = load_data('./data/train.csv')\n",
      "y = load_data('./data/trainLabels.csv')\n",
      "test = load_data('./data/test.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def divid_data(X_, y_):\n",
      "    #randomly divide the data into train and test set, the test size is 10% of the whole dataset\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.3)\n",
      "    return X_train, y_train, X_test, y_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 154
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##First try to see the 2 main components of the data (Visualize all data)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca2 = PCA(n_components=2, whiten=True)\n",
      "pca2.fit(np.r_[X, test])\n",
      "X_pca = pca2.transform(X)\n",
      "i0 = np.argwhere(y == 0)[:, 0]\n",
      "i1 = np.argwhere(y == 1)[:, 0]\n",
      "X0 = X_pca[i0, :]\n",
      "X1 = X_pca[i1, :]\n",
      "plt.plot(X0[:, 0], X0[:, 1], 'ro')\n",
      "plt.plot(X1[:, 0], X1[:, 1], 'b*')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Let's see each feature explained how much variance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = PCA(whiten=True)\n",
      "X_all = pca.fit_transform(np.r_[X, test])\n",
      "print pca.explained_variance_ratio_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  2.61338893e-01   2.03953972e-01   7.91104009e-02   4.81480076e-02\n",
        "   4.54167442e-02   4.42563499e-02   4.04285747e-02   3.03524876e-02\n",
        "   2.35202947e-02   1.92164610e-02   1.61483800e-02   1.25565492e-02\n",
        "   7.65669101e-03   7.59267345e-03   7.57067587e-03   7.45196972e-03\n",
        "   7.39437803e-03   7.33602203e-03   7.32154020e-03   7.26446366e-03\n",
        "   7.24687802e-03   7.13785979e-03   7.08554112e-03   7.05759361e-03\n",
        "   7.01144882e-03   6.98357997e-03   6.90865448e-03   6.88494883e-03\n",
        "   6.84557325e-03   6.83367011e-03   6.74339150e-03   6.70112785e-03\n",
        "   6.63533428e-03   6.57374179e-03   6.49188079e-03   6.46379503e-03\n",
        "   6.35945185e-03   2.11189295e-31   3.63031139e-32   1.43643870e-32]\n"
       ]
      }
     ],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##plot each component explained how much variance\n",
      "plt.plot(pca.explained_variance_ratio_)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 141
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X_all[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.51017127 -1.41089071 -0.85071073  0.3560015  -1.14357535 -0.11125238\n",
        "  1.85135456 -1.09820807  1.96900898  0.63831842 -0.99654363 -0.02979864\n",
        " -0.3980307   0.3568081   1.02811663 -0.62077649 -0.68981988 -0.18859586\n",
        " -1.83914457 -0.46683028  1.24110721 -0.54904963 -2.12109335 -0.67868514\n",
        " -0.41267871 -1.01414606  0.4162249  -1.76965159 -0.81326271 -2.50610655\n",
        "  1.25913002  0.82738199 -0.13427502 -1.05439425 -1.25976368 -0.66387921\n",
        " -0.44830517  4.50996547 -1.78882533 -3.3270395 ]\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print np.r_[X, test][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.29940251 -1.22662419  1.49842505 -1.17615036  5.28985255  0.20829711\n",
        "  2.40449837  1.59450622 -0.05160816  0.66323431 -1.40837006  1.11474364\n",
        "  0.91041531  2.21811032  4.30564273  0.08892398  0.16914926  0.41344764\n",
        "  1.51386217  2.66296658 -1.07276548  0.14911112  0.55957919  4.37888464\n",
        " -0.46360266 -0.06395916  0.54493031  0.71277167 -1.49405013 -2.63616888\n",
        " -0.85046545 -0.62299    -1.83305734  0.29302439  3.55268134  0.71761099\n",
        "  3.30597197 -2.71555881 -2.68240859  0.10105047]\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##We can see from the above features, that the first 12 components explained most of the variance"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Now let's plot the KDE and QQ plots"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def kde_plot(x):\n",
      "        from scipy.stats.kde import gaussian_kde\n",
      "        kde = gaussian_kde(x)\n",
      "        positions = np.linspace(x.min(), x.max())\n",
      "        smoothed = kde(positions)\n",
      "        plt.plot(positions, smoothed)\n",
      "        plt.show()\n",
      "\n",
      "def qq_plot(x):\n",
      "    from scipy.stats import probplot\n",
      "    probplot(x, dist='norm', plot=plt)\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kde_plot(X_all[:, 7])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kde_plot(X_all[:, 23])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kde_plot(X_all[:, 39])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##It does look like the features come from gaussian distribution. And then plot the QQ plot. In statistics, a Q\u2013Q plot (\"Q\" stands for quantile) is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "qq_plot(X[:,7])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "qq_plot(X[:,23])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "qq_plot(X[:,39])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Use Gaussian mixture model to model the data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#You can see from the explained_variance plot, 12 is the best number to use\n",
      "#this is just a test, and all the parameters are got from Luan Junyi, if it is working\n",
      "#I will try a grid search to find the best one, but I think these are quite close to \n",
      "#the optimum, because he tried grid search to find these ones?\n",
      "pca_components = 12\n",
      "gmm_components = 4\n",
      "covariance_type = \"full\"\n",
      "min_covar = 0.1\n",
      "gamma = 0\n",
      "C = 1.0\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#let's only use test data as the training set to estimate the gmm, \n",
      "#and then feed the training data to gmm, which generates the probabilty of the \n",
      "#data point belong to which gaussian, and then feed this probability to svm to train the classifier\n",
      "\n",
      "#using all the test data to estimate the parameters of the gmm\n",
      "X_all = pca.fit_transform(test)[:, :pca_components]\n",
      "\n",
      "print len(X_all)\n",
      "gmm = GMM(n_components = gmm_components,\n",
      "                       covariance_type = covariance_type,\n",
      "                       min_covar = min_covar)\n",
      "gmm.fit(X_all)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9000\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 161,
       "text": [
        "GMM(covariance_type='full', init_params='wmc', min_covar=0.1, n_components=4,\n",
        "  n_init=1, n_iter=100, params='wmc', random_state=None, thresh=0.01)"
       ]
      }
     ],
     "prompt_number": 161
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Now let's feed the training data to the gmm, and generates the probability \n",
      "X_tmp = pca.transform(X)[:, :pca_components]\n",
      "X_t = gmm.predict_proba(X_tmp)\n",
      "\n",
      "#then divide this new data set to training and testing again\n",
      "X_train, y_train, X_test, y_test = divid_data(X_t, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 167
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# fit the model with different kernels\n",
      "for fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):\n",
      "    clf = svm.SVC(kernel=kernel, gamma=10)\n",
      "    print('svm with ' + kernel + ' kernel score: %f' % clf.fit(X_train, y_train).score(X_test, y_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "svm with linear kernel score: 0.996667\n",
        "svm with rbf kernel score: 0.990000\n",
        "svm with poly kernel score: 0.993333\n"
       ]
      }
     ],
     "prompt_number": 168
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}